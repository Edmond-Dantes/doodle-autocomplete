{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295c77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipeline = pipeline(\n",
    "#     task=\"image-classification\",\n",
    "#     model=\"google/vit-base-patch16-224\",\n",
    "#     device=-1,\n",
    "# )\n",
    "# pipeline(\n",
    "#     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7f369",
   "metadata": {},
   "source": [
    "Step-by-Step: Fine-Tuning ViT on Quick, Draw!\n",
    "1. Install and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c3705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quickdraw import QuickDrawDataGroup\n",
    "\n",
    "# # Example: load 1000 recognized circle drawings\n",
    "# qdg = QuickDrawDataGroup(\"circle\", recognized=True, max_drawings=1000)\n",
    "# images = [d.image.convert(\"RGB\") for d in qdg.drawings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748abc3",
   "metadata": {},
   "source": [
    "2. Label & Organize\n",
    "\n",
    "Assign numeric labels for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5200922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading circle drawings\n",
      "load complete\n",
      "loading square drawings\n",
      "load complete\n",
      "downloading line from https://storage.googleapis.com/quickdraw_dataset/full/binary/line.bin\n",
      "download complete\n",
      "loading line drawings\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "from quickdraw import QuickDrawDataGroup\n",
    "\n",
    "categories = [\"circle\", \"square\", \"line\"]  # customize as needed\n",
    "label_map = {name: i for i, name in enumerate(categories)}\n",
    "\n",
    "# Collect images and labels\n",
    "all_images, all_labels = [], []\n",
    "for name in categories:\n",
    "    group = QuickDrawDataGroup(name, recognized=True, max_drawings=300)\n",
    "    for d in group.drawings:\n",
    "        all_images.append(d.image.convert(\"RGB\"))\n",
    "        all_labels.append(label_map[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23859b",
   "metadata": {},
   "source": [
    "3. Create Dataset (via Hugging Face datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62d33f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edmond/.pyenv/versions/3.10.6/envs/edmond-doodle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "output_dir = \"shapes_dataset/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "paths, labels = [], []\n",
    "\n",
    "for i, (img, label) in enumerate(zip(all_images, all_labels)):\n",
    "    path = os.path.join(output_dir, f\"img_{i}.png\")\n",
    "    img.save(path)\n",
    "    paths.append(path)\n",
    "    labels.append(label)\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"image\": paths, \"label\": labels})  # Use paths instead of all_images\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e13f0",
   "metadata": {},
   "source": [
    "4. Preprocess with AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891925d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Map: 100%|██████████| 720/720 [00:06<00:00, 119.86 examples/s]\n",
      "Map: 100%|██████████| 180/180 [00:01<00:00, 126.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Load images from paths\n",
    "    images = [Image.open(img_path).convert(\"RGB\") for img_path in examples[\"image\"]]\n",
    "    # Process images and return pixel values\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    examples[\"pixel_values\"] = inputs[\"pixel_values\"]\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "ds = ds.map(preprocess_function, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae43eae",
   "metadata": {},
   "source": [
    "5. Load Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f1116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=len(categories),\n",
    "    id2label={i: name for name, i in label_map.items()},\n",
    "    label2id=label_map,\n",
    "    ignore_mismatched_sizes=True  # Ignore size mismatch for classification head\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c2e05",
   "metadata": {},
   "source": [
    "6. Train with Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbe13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/thprg65n1798px0d5lscm0xm0000gn/T/ipykernel_44829/2927564075.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 24:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.011844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.017189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.014510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=135, training_loss=0.04382045715012484, metrics={'train_runtime': 1475.4562, 'train_samples_per_second': 1.464, 'train_steps_per_second': 0.091, 'total_flos': 1.6738419776569344e+17, 'train_loss': 0.04382045715012484, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DefaultDataCollator\n",
    "\n",
    "# Use the default data collator which handles tensors properly\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-quickdraw\",\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bec74",
   "metadata": {},
   "source": [
    "7. Evaluate & Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6a9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'your_pil_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel, feature_extractor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[0;32m----> 3\u001b[0m clf(\u001b[43myour_pil_image\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'your_pil_image' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "clf = pipeline(\"image-classification\", model=trainer.model, feature_extractor=processor)\n",
    "clf(your_pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f803881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to './vit-quickdraw-final'\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./vit-quickdraw-final\")\n",
    "\n",
    "print(\"Model saved to './vit-quickdraw-final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb821e",
   "metadata": {},
   "source": [
    "# Load Model and Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05f03e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edmond/.pyenv/versions/3.10.6/envs/edmond-doodle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load model and processor\n",
    "model_path = './vit-quickdraw-final'\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def classify_doodle(image):\n",
    "    \"\"\"\n",
    "    Classify a doodle image\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array or path to image file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_class, confidence_score, all_probabilities)\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    # Ensure image is RGB\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Process the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class_id = probabilities.argmax().item()\n",
    "        confidence = probabilities.max().item()\n",
    "    \n",
    "    # Get class labels and probabilities\n",
    "    predicted_label = model.config.id2label[str(predicted_class_id)]\n",
    "    all_probs = {model.config.id2label[str(i)]: prob.item() \n",
    "                 for i, prob in enumerate(probabilities[0])}\n",
    "    \n",
    "    return predicted_label, confidence, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f448e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85800963"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341a99a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_doodle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mclassify_doodle\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Handle different input types\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     26\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/edmond-doodle/lib/python3.10/site-packages/PIL/Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg'"
     ]
    }
   ],
   "source": [
    "test_image_path = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "result = classify_doodle(test_image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b090a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'circle', 'score': 0.40838727355003357},\n",
       " {'label': 'line', 'score': 0.3109080195426941},\n",
       " {'label': 'square', 'score': 0.2807047367095947}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "your_pil_image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "clf = pipeline(\"image-classification\", model=model, feature_extractor=processor)\n",
    "clf(your_pil_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8414245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'line', 'score': 0.9834213852882385},\n",
       " {'label': 'circle', 'score': 0.012197574600577354},\n",
       " {'label': 'square', 'score': 0.004381043836474419}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf(\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcreazilla-store.fra1.digitaloceanspaces.com%2Fcliparts%2F7822516%2Fhand-drawn-circles-clipart-md.png&f=1&nofb=1&ipt=3dde44d393119c19cd53efe09d82655be0eca574b56726fe4148de728d49ee24\")\n",
    "# clf(\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcontent.clipchamp.com%2Fcontent-repo%2Fcontent%2Fpreviews%2Fcc_ea807c5a.png&f=1&nofb=1&ipt=cc8ab3d317a4a685990a4ee9b588014c37391bd11e7c77cb44c718ed16ee4679\")\n",
    "# clf(\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn1.vectorstock.com%2Fi%2F1000x1000%2F65%2F35%2Fhand-drawn-circle-line-sketch-set-circular-vector-26936535.jpg&f=1&nofb=1&ipt=6b73329471feaa59e4f85137f0fca30aba8575629952e782fe86703cfe558511\")\n",
    "clf(\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fas2.ftcdn.net%2Fv2%2Fjpg%2F05%2F52%2F34%2F03%2F1000_F_552340334_TXgPQTLmEPSlyJ6mZ1S6ixCZpmE4dvpV.jpg&f=1&nofb=1&ipt=655e28b444d76496d73cd20b9d80befefd25b4e11d822fed0197e183d40c3d78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209b54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmond-doodle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
