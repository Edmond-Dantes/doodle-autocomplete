{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42a5dce",
   "metadata": {},
   "source": [
    "# Contents of this Notebook\n",
    "• Quick look at some images in the dataset\n",
    "\n",
    "• Download a few categories of shape positives and some categories to distinguish those from\n",
    "\n",
    "• Download the classification text file\n",
    "\n",
    "• Set some core variables needed throughout\n",
    "\n",
    "• Load flattened arrays (N, 784) → reshape to (N, 28, 28)\n",
    "\n",
    "• Organise data into X and y\n",
    "\n",
    "• Define and compile the model\n",
    "\n",
    "• Test train split\n",
    "\n",
    "• Training\n",
    "\n",
    "• Preview, graphs, refinement\n",
    "\n",
    "• Saving the model and converting it into a javascript model (tensorflowjs) via terminal commands\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "081707ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from quickdraw import QuickDrawDataGroup\n",
    "import os, urllib.request\n",
    "import requests\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json, pathlib\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from scipy.ndimage import rotate\n",
    "import tensorflow as tf\n",
    "from math import sin, cos, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8098b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Resolve full path to service account JSON\n",
    "key_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if not key_path:\n",
    "    raise ValueError(\"GOOGLE_APPLICATION_CREDENTIALS not set in .env\")\n",
    "\n",
    "key_path = os.path.abspath(os.path.expanduser(key_path))\n",
    "config = dotenv_values(\".env\").get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_group = QuickDrawDataGroup(\"circle\", max_drawings=10000)\n",
    "\n",
    "for _ in range(5):\n",
    "    d = circle_group.get_drawing()\n",
    "    display(d.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5633d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"  # where your .npy files are\n",
    "\n",
    "TARGETS = [\"circle\", \"square\", \"triangle\", \"star\", \"vertical_arrows\", \"axis\"]\n",
    "OTHERS  = [\"face\", \"cat\", \"cloud\", \"house\", \"tree\"]\n",
    "\n",
    "CLASS_NAMES = TARGETS + [\"other\"]\n",
    "CLASS_TO_ID = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "\n",
    "classes = TARGETS + OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0794662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ✅\n"
     ]
    }
   ],
   "source": [
    "# Assumes you've already authenticated using your .env and service account key\n",
    "# and set GOOGLE_APPLICATION_CREDENTIALS properly before this code runs\n",
    "\n",
    "bucket_name = \"doodle-autocomplete\"\n",
    "prefix = \"training-data\"\n",
    "\n",
    "# Make the data directory if you don't have it\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Create GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "for c in classes:\n",
    "    blob_path = f\"{prefix}/{c}.npy\"\n",
    "    dst = os.path.join(DATA_DIR, f\"{c}.npy\")\n",
    "\n",
    "    if not os.path.exists(dst):\n",
    "        print(f\"Downloading {blob_path} from GCS...\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.download_to_filename(dst)\n",
    "\n",
    "print(\"Done ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782911f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull from official QuickDraw data\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True) #make the data directory if you don't have it on your machine\n",
    "\n",
    "base = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap\"\n",
    "\n",
    "for c in classes:\n",
    "    url = f\"{base}/{c}.npy\"\n",
    "    dst = os.path.join(DATA_DIR, f\"{c}.npy\")\n",
    "    if not os.path.exists(dst):\n",
    "        print(\"downloading\", url)\n",
    "        urllib.request.urlretrieve(url, dst)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d03e0f",
   "metadata": {},
   "source": [
    "# 1. About the data - What Google did with QuickDraw\n",
    "\n",
    "Each QuickDraw doodle was drawn by a person as a sequence of strokes (vector data).\n",
    "\n",
    "Google rasterised those strokes into 28 × 28 pixel images, grayscale.\n",
    "\n",
    "Each pixel is **grayscale** — it holds an intensity value from 0 to 255 (0 = black, 255 = white, or sometimes inverted depending on convention).\n",
    "\n",
    "To make the dataset compact, they then flattened each 28×28 image into a 1D vector of length 784 (because 28 × 28 = 784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e7376ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(path, limit=None):\n",
    "    \"\"\"\n",
    "    Quick, Draw! numpy_bitmap .npy: shape (N, 784) uint8.\n",
    "    Returns (N, 28, 28) uint8.\n",
    "    \"\"\"\n",
    "    arr = np.load(path)                 # (N, 784) uint8\n",
    "    if limit is not None:\n",
    "        arr = arr[:limit]\n",
    "    return arr.reshape(-1, 28, 28)      # -> (N,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e6d1b",
   "metadata": {},
   "source": [
    "# Logic of the data distribution in the model\n",
    "Some false positive classes were included to train the model - it needs to learn not only to recognise circles and squares but also to disregard random stuff. The random stuff included in this model is:\n",
    "\n",
    " \"face\", \"cat\", \"cloud\", \"house\", \"tree\"\n",
    "\n",
    " But this could be increased to make a better model.\n",
    "\n",
    " Below the function therefore deals with both of those.\n",
    "\n",
    "\n",
    "# What the function returns\n",
    "\n",
    "X: all images, normalised to [0,1] and shaped (N, 28, 28, 1) (float32).\n",
    "\n",
    "y: one integer label per image, shaped (N,) (int64). Uses your CLASS_TO_ID mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21263752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_arrays(data_dir, per_class=10000, other_total=5000, seed=0):\n",
    "#     rng = np.random.default_rng(seed)  # for a reproducible final shuffle\n",
    "\n",
    "#     Xs, ys = [], []  # we collect per-class arrays here (features X, labels y)\n",
    "\n",
    "#     # 1) TARGET classes: circle/square/triangle/star\n",
    "#     for cname in TARGETS:\n",
    "#         Xc = load_npy(os.path.join(data_dir, f\"{cname}.npy\"), per_class)\n",
    "#         # Xc: (per_class, 28, 28) uint8 — the images for this target class\n",
    "#         yc = np.full(len(Xc), CLASS_TO_ID[cname], dtype=np.int64)\n",
    "#         # yc: (per_class,) int64 — the labels for this class, all the same id\n",
    "#         Xs.append(Xc)\n",
    "#         ys.append(yc)\n",
    "\n",
    "#     # 2) \"OTHER\" bucket: pool several categories into one label\n",
    "#     per_other = max(1, other_total // len(OTHERS))  # try to sample evenly\n",
    "#     other_parts = []\n",
    "#     for cname in OTHERS:\n",
    "#         Xo_part = load_npy(os.path.join(data_dir, f\"{cname}.npy\"), per_other)\n",
    "#         other_parts.append(Xo_part)\n",
    "#     Xo = np.concatenate(other_parts, axis=0)             # (~other_total, 28, 28)\n",
    "#     yo = np.full(len(Xo), CLASS_TO_ID[\"other\"], dtype=np.int64)  # same id for all\n",
    "\n",
    "#     # 3) Concatenate everything into single feature/label arrays\n",
    "#     X = np.concatenate(Xs + [Xo], axis=0)  # (N, 28, 28) uint8\n",
    "#     y = np.concatenate(ys + [yo], axis=0)  # (N,)        int64\n",
    "\n",
    "#     # 4) Normalise & add channel dim for Keras (channels-last)\n",
    "#     X = (X.astype(\"float32\") / 255.0)[..., None]  # (N, 28, 28, 1) float32\n",
    "\n",
    "#     # 5) Final shuffle to mix classes so batches aren’t blocky\n",
    "#     idx = rng.permutation(len(X))\n",
    "#     return X[idx], y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arrays(data_dir, per_class=10000, other_total=5000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    Xs = []\n",
    "    ys_class = []\n",
    "    ys_orient = []\n",
    "\n",
    "    for cname in TARGETS:\n",
    "        path = os.path.join(data_dir, f\"{cname}.npy\")\n",
    "\n",
    "        if cname == \"vertical_arrows\":\n",
    "            max_samples = 500      # number of vertical arrows to sample\n",
    "            rotations_per_img = 24 # rotate each selected image 24 ways\n",
    "\n",
    "            X_base = load_npy(path)  # load all vertical arrow images\n",
    "\n",
    "            # Subsample vertical arrows randomly\n",
    "            if len(X_base) > max_samples:\n",
    "                X_base = rng.choice(X_base, size=max_samples, replace=False)\n",
    "\n",
    "            Xc, yo = [], []\n",
    "            for img in X_base:\n",
    "                angles = np.linspace(0, 360, rotations_per_img, endpoint=False)\n",
    "                for angle in angles:\n",
    "                    rotated_img = rotate(img, angle, reshape=False, order=1, mode='constant', cval=0.0)\n",
    "                    Xc.append(rotated_img)\n",
    "                    yo.append(angle)\n",
    "\n",
    "            Xc = np.array(Xc)\n",
    "            yc = np.full(len(Xc), CLASS_TO_ID[cname], dtype=np.int64)\n",
    "            yo = np.array(yo).astype(np.float32)\n",
    "\n",
    "            yo_sin_cos = np.zeros((len(yo), 2), dtype=np.float32)\n",
    "            yo_rad = np.deg2rad(yo)          # convert degrees to radians\n",
    "            yo_sin_cos[:, 0] = np.sin(yo_rad)\n",
    "            yo_sin_cos[:, 1] = np.cos(yo_rad)\n",
    "            yo = yo_sin_cos                  # replace yo with sin/cos\n",
    "\n",
    "        else:\n",
    "            Xc = load_npy(path, limit=per_class)\n",
    "            yc = np.full(len(Xc), CLASS_TO_ID[cname], dtype=np.int64)\n",
    "            yo = np.full((len(Xc), 2), -999.0, dtype=np.float32) #setting masking values for sin and cos angle of \"other\" class\n",
    "\n",
    "        Xs.append(Xc)\n",
    "        ys_class.append(yc)\n",
    "        ys_orient.append(yo)\n",
    "\n",
    "    # Process OTHER classes as before\n",
    "    per_other = max(1, other_total // len(OTHERS))\n",
    "    other_parts = []\n",
    "    for cname in OTHERS:\n",
    "        path = os.path.join(data_dir, f\"{cname}.npy\")\n",
    "        Xo_part = load_npy(path, limit=per_other)\n",
    "        other_parts.append(Xo_part)\n",
    "\n",
    "    Xo = np.concatenate(other_parts, axis=0)\n",
    "    yo = np.full((len(Xo), 2), -999.0, dtype=np.float32)\n",
    "    yc = np.full(len(Xo), CLASS_TO_ID[\"other\"], dtype=np.int64)\n",
    "\n",
    "    Xs.append(Xo)\n",
    "    ys_class.append(yc)\n",
    "    ys_orient.append(yo)\n",
    "\n",
    "    # Concatenate all data\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y_class = np.concatenate(ys_class, axis=0)\n",
    "    y_orient = np.concatenate(ys_orient, axis=0)\n",
    "\n",
    "    # Normalize and add channel dim\n",
    "    X = (X.astype(\"float32\") / 255.0)[..., None]\n",
    "\n",
    "    # Shuffle\n",
    "    idx = rng.permutation(len(X))\n",
    "    return X[idx], y_class[idx], y_orient[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75ffa9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37511, 28, 28, 1),\n",
       " (37511,),\n",
       " (37511, 2),\n",
       " array([ 5000,  5000,  5000,  5000, 12000,   511,  5000]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y_class, y_orient = build_arrays(DATA_DIR, per_class=5000, other_total=5000)  # ~15k total\n",
    "X.shape, y_class.shape, y_orient.shape, np.bincount(y_class)\n",
    "# Expect: ((~15000, 28, 28, 1), (~15000,), counts per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45644632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is to create augmented data for more variety in the training samples during model fitting.\n",
    "rotation_layer = tf.keras.layers.RandomRotation(\n",
    "    factor=0.15,  # ±15% of a full rotation\n",
    "    fill_mode='constant'\n",
    ")\n",
    "\n",
    "def augment_fn(img, y_class, y_orient):\n",
    "    if tf.equal(y_class, CLASS_TO_ID[\"vertical_arrows\"]):\n",
    "        # no augmentation for vertical-line\n",
    "        return img, (y_class, y_orient)\n",
    "    else:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        # apply RandomRotation layer\n",
    "        img = rotation_layer(tf.expand_dims(img, axis=0))  # add batch dim\n",
    "        img = tf.squeeze(img, axis=0)                     # remove batch dim\n",
    "        return img, (y_class, y_orient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32896d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)       [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 28, 28, 16)           160       ['input_19[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_36 (MaxPooli  (None, 14, 14, 16)           0         ['conv2d_36[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 14, 14, 32)           4640      ['max_pooling2d_36[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_37 (MaxPooli  (None, 7, 7, 32)             0         ['conv2d_37[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " flatten_18 (Flatten)        (None, 1568)                 0         ['max_pooling2d_37[0][0]']    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 64)                   100416    ['flatten_18[0][0]']          \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 32)                   2080      ['dense_35[0][0]']            \n",
      "                                                                                                  \n",
      " class_output (Dense)        (None, 7)                    455       ['dense_35[0][0]']            \n",
      "                                                                                                  \n",
      " orient_output (Dense)       (None, 2)                    66        ['dense_36[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 107817 (421.16 KB)\n",
      "Trainable params: 107817 (421.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(CLASS_NAMES)\n",
    "\n",
    "# Input layer\n",
    "inputs = layers.Input(shape=(28,28,1))\n",
    "\n",
    "# Shared CNN\n",
    "x = layers.Conv2D(16,3,padding=\"same\",activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Conv2D(32,3,padding=\"same\",activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "shared_dense = layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "# Classification branch\n",
    "class_output = layers.Dense(n_classes, activation=\"softmax\", name=\"class_output\")(shared_dense)\n",
    "\n",
    "# Orientation branch\n",
    "orient_branch = layers.Dense(32, activation=\"relu\")(shared_dense)\n",
    "orient_output = layers.Dense(2, activation=\"linear\", name=\"orient_output\")(orient_branch)\n",
    "\n",
    "# Create the model with two outputs\n",
    "model = models.Model(inputs=inputs, outputs=[class_output, orient_output], name=\"classifier\")\n",
    "\n",
    "\n",
    "# custom loss function that looks at the difference in angles\n",
    "def angle_loss(y_true, y_pred):\n",
    "    # mask invalid targets\n",
    "    mask = tf.reduce_sum(tf.abs(y_true + 999), axis=1) > 0\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "    # if no valid angles, return 0\n",
    "    def valid_loss():\n",
    "        # normalize predictions to unit length\n",
    "        y_pred_norm = tf.nn.l2_normalize(y_pred_masked, axis=1)\n",
    "        cos_diff = tf.reduce_sum(y_true_masked * y_pred_norm, axis=1)\n",
    "        return tf.reduce_mean(1 - cos_diff)\n",
    "\n",
    "    return tf.cond(tf.size(y_true_masked) > 0, valid_loss, lambda: 0.0)\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss={\n",
    "                  \"class_output\": \"sparse_categorical_crossentropy\",\n",
    "                  \"orient_output\": angle_loss\n",
    "              },\n",
    "              loss_weights={\n",
    "                  \"class_output\": 1.0,\n",
    "                  \"orient_output\": 0.5,\n",
    "              },\n",
    "              metrics={\n",
    "                  \"class_output\": \"accuracy\",\n",
    "                  \"orient_output\": \"mae\"\n",
    "              })\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f25764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = len(CLASS_NAMES)\n",
    "\n",
    "# model = Sequential([\n",
    "#     layers.Input(shape=(28,28,1)),\n",
    "#     layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "#     layers.MaxPooling2D(2),                # 28→14\n",
    "#     layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "#     layers.MaxPooling2D(2),                # 14→7\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation=\"relu\"),\n",
    "#     layers.Dense(n_classes, activation=\"softmax\"),\n",
    "# ])\n",
    "\n",
    "# # 2) augmentation pipeline\n",
    "# data_augmentation = models.Sequential([\n",
    "#     layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "#     layers.RandomRotation(0.15, fill_mode=\"constant\", fill_value=0.0),\n",
    "# ], name=\"aug\")\n",
    "\n",
    "# # 3) training model = aug + model\n",
    "# train_model = models.Sequential([\n",
    "#     layers.Input(shape=(28,28,1)),\n",
    "#     data_augmentation,\n",
    "#     model\n",
    "# ], name=\"train_model\")\n",
    "\n",
    "# model.compile(optimizer=\"Adam\",\n",
    "#               loss=\"sparse_categorical_crossentropy\",\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a188b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'max_pooling2d_36' (type MaxPooling2D).\n    \n    Negative dimension size caused by subtracting 2 from 1 for '{{node classifier/max_pooling2d_36/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](classifier/conv2d_36/Relu)' with input shapes: [28,28,1,16].\n    \n    Call arguments received by layer 'max_pooling2d_36' (type MaxPooling2D):\n      • inputs=tf.Tensor(shape=(28, 28, 1, 16), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/05/bxx2v3ss5v56_6y10k6wzfnh0000gp/T/__autograph_generated_file9m4leko6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mayaamaryllis/.pyenv/versions/3.11.1/envs/doodle-autocomplete/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'max_pooling2d_36' (type MaxPooling2D).\n    \n    Negative dimension size caused by subtracting 2 from 1 for '{{node classifier/max_pooling2d_36/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](classifier/conv2d_36/Relu)' with input shapes: [28,28,1,16].\n    \n    Call arguments received by layer 'max_pooling2d_36' (type MaxPooling2D):\n      • inputs=tf.Tensor(shape=(28, 28, 1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Split the data and both labels; stratify on class labels\n",
    "Xtr, Xva, y_class_tr, y_class_va, y_orient_tr, y_orient_va = train_test_split(\n",
    "    X, y_class, y_orient, test_size=0.1, random_state=39, stratify=y_class\n",
    ")\n",
    "\n",
    "# Create tf.data.Dataset objects\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, (y_class_tr, y_orient_tr)))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((Xva, (y_class_va, y_orient_va)))\n",
    "\n",
    "# # Apply conditional augmentation only to training dataset\n",
    "# train_ds = train_ds.map(\n",
    "#     lambda x, y: augment_fn(x, y[0], y[1]),\n",
    "#     num_parallel_calls=tf.data.AUTOTUNE\n",
    "# )\n",
    "\n",
    "# Shuffle, batch, and prefetch\n",
    "train_ds = train_ds.shuffle(2048).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = val_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "es = EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "model.fit(\n",
    "    X,\n",
    "    {\"class_output\": y_class, \"orient_output\": y_orient},\n",
    "    batch_size=64,\n",
    "    epochs=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74784581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(X, y, n=10):\n",
    "    plt.figure(figsize=(12,2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(X[i,...,0], cmap=\"gray\")\n",
    "        plt.title(CLASS_NAMES[y[i]], fontsize=9)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "show_grid(Xtr, ytr, n=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on validation set\n",
    "proba = model.predict(Xva, verbose=0)\n",
    "yhat  = proba.argmax(axis=1)\n",
    "\n",
    "print(classification_report(yva, yhat, target_names=CLASS_NAMES, digits=3))\n",
    "\n",
    "# confusion matrix (vanilla matplotlib)\n",
    "cm = confusion_matrix(yva, yhat, labels=range(len(CLASS_NAMES)))\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "im = ax.imshow(cm, interpolation='nearest')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_xticks(range(len(CLASS_NAMES))); ax.set_yticks(range(len(CLASS_NAMES)))\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(CLASS_NAMES)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58566182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_reject(Xb, tau=0.6):\n",
    "    proba = model.predict(Xb, verbose=0)\n",
    "    maxp  = proba.max(axis=1)\n",
    "    arg   = proba.argmax(axis=1)\n",
    "    labels = np.where(maxp < tau, -1, arg)  # -1 = unknown\n",
    "    return labels, maxp, proba\n",
    "\n",
    "# sweep τ to see trade-off\n",
    "taus = np.linspace(0.4, 0.9, 11)\n",
    "accs = []\n",
    "for t in taus:\n",
    "    labels, _, _ = predict_with_reject(Xva, tau=t)\n",
    "    # treat unknown as wrong for this simple score\n",
    "    ok = (labels == yva)\n",
    "    accs.append(ok.mean())\n",
    "\n",
    "plt.figure(); plt.plot(taus, accs, marker=\"o\")\n",
    "plt.xlabel(\"tau (rejection threshold)\"); plt.ylabel(\"val accuracy (unknown counted as wrong)\")\n",
    "plt.title(\"Pick a τ you like\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a few that weren't in TARGETS or OTHERS\n",
    "UNSEEN = [\"airplane\",\"banana\",\"bus\",\"chair\"]\n",
    "unseen_batch = []\n",
    "for c in UNSEEN:\n",
    "    path = os.path.join(DATA_DIR, f\"{c}.npy\")\n",
    "    if os.path.exists(path):\n",
    "        arr = np.load(path)[:200].reshape(-1,28,28,1).astype(\"float32\")/255.0\n",
    "        unseen_batch.append(arr)\n",
    "if unseen_batch:\n",
    "    X_unseen = np.concatenate(unseen_batch, axis=0)\n",
    "    labels, maxp, _ = predict_with_reject(X_unseen, tau=0.6)\n",
    "    unknown_rate = (labels==-1).mean()\n",
    "    print(f\"Unknown rate on unseen classes: {unknown_rate:.2%} (want this fairly high)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d887a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_one_live(img28, tau=0.6, mdl=model, classes=CLASS_NAMES):\n",
    "    x = img28.astype(\"float32\")\n",
    "    if x.max() > 1.0:\n",
    "        x = x / 255.0\n",
    "    x = x[None, ..., None]\n",
    "    proba = mdl.predict(x, verbose=0)[0]\n",
    "    i = proba.argmax()\n",
    "    return (\"unknown\", float(proba[i])) if proba[i] < tau else (classes[i], float(proba[i]))\n",
    "\n",
    "\n",
    "# demo on a val sample\n",
    "lbl, conf = predict_one_live((Xva[0,...,0]*255).astype(\"uint8\"), tau=0.6)\n",
    "lbl, conf, \"true:\", CLASS_NAMES[yva[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62458205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_and_show(img_flat, model, class_names=CLASS_NAMES):\n",
    "    \"\"\"\n",
    "    Predicts the class and orientation of a Quick, Draw! sample\n",
    "    and displays the image.\n",
    "\n",
    "    Parameters:\n",
    "        img_flat: np.ndarray, shape (784,) — flattened image\n",
    "        model: trained Keras model with two outputs\n",
    "        class_names: list of class names, matching model output\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    img = img_flat.reshape(28,28).astype(\"float32\") / 255.0  # normalize\n",
    "    img = np.expand_dims(img, axis=-1)  # add channel dim (28,28,1)\n",
    "    img = np.expand_dims(img, axis=0)   # add batch dim (1,28,28,1)\n",
    "\n",
    "    # Predict\n",
    "    class_pred, orient_pred = model.predict(img, verbose=0)\n",
    "    pred_class_idx = np.argmax(class_pred, axis=1)[0]\n",
    "    pred_class_name = class_names[pred_class_idx]\n",
    "    pred_orientation = orient_pred[0][0]\n",
    "\n",
    "    angle_pred_rad = np.arctan2(orient_pred[0][0], orient_pred[0][1])\n",
    "    angle_pred_deg = np.degrees(angle_pred_rad) % 360\n",
    "\n",
    "    # Show image\n",
    "    plt.imshow(img[0,...,0], cmap='gray')\n",
    "    plt.title(f\"Predicted: {pred_class_name}\\nOrientation: {angle_pred_deg:.1f}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return pred_class_name, angle_pred_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae35398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jitter the training images by ±1 pixel\n",
    "def jitter_shift(x):\n",
    "    dy = np.random.randint(-1, 2)\n",
    "    dx = np.random.randint(-1, 2)\n",
    "    return np.roll(np.roll(x, dy, axis=0), dx, axis=1)\n",
    "\n",
    "aug_X = Xtr.copy()\n",
    "mask = np.random.rand(len(aug_X)) < 0.5\n",
    "aug_X[mask,...,0] = np.array([jitter_shift(im) for im in aug_X[mask,...,0]])\n",
    "\n",
    "#history = model.fit(aug_X, ytr, validation_data=(Xva, yva), epochs=6, batch_size=32, callbacks=[es], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7a53323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_and_show_rotated(img_rotated, model, class_names=CLASS_NAMES):\n",
    "    \"\"\"\n",
    "    Predict class and orientation of a rotated Quick, Draw! image and display it with an arrow.\n",
    "\n",
    "    Parameters:\n",
    "        img_rotated: np.ndarray, shape (28,28) — already rotated image\n",
    "        model: trained Keras model with two outputs\n",
    "        class_names: list of class names\n",
    "\n",
    "    Returns:\n",
    "        pred_class_name: str\n",
    "        pred_orientation: float (degrees)\n",
    "    \"\"\"\n",
    "    # Normalize and add channel/batch dimensions\n",
    "    img_input = img_rotated.astype(\"float32\") / 255.0  # normalize\n",
    "    img_input = img_input[..., None]                  # (28,28,1)\n",
    "    img_input = np.expand_dims(img_input, axis=0)     # (1,28,28,1)\n",
    "\n",
    "    # Predict\n",
    "    class_pred, orient_pred = model.predict(img_input, verbose=0)\n",
    "    pred_class_idx = np.argmax(class_pred, axis=1)[0]\n",
    "    pred_class_name = class_names[pred_class_idx]\n",
    "    pred_orientation = orient_pred[0][0]\n",
    "\n",
    "    # Display\n",
    "    plt.imshow(img_rotated, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{pred_class_name}\\nPredicted orientation: {pred_orientation:.1f}°\")\n",
    "\n",
    "    # Draw arrow\n",
    "    center = (14,14)\n",
    "    length = 10\n",
    "    angle_rad = np.deg2rad(pred_orientation)\n",
    "    dx = length * np.cos(angle_rad)\n",
    "    dy = -length * np.sin(angle_rad)  # negative because image y-axis is top→bottom\n",
    "    plt.arrow(center[0], center[1], dx, dy, color='red', head_width=2, head_length=3)\n",
    "    plt.show()\n",
    "\n",
    "    return pred_class_name, pred_orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d68a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Load vertical_arrows class\n",
    "arr = np.load(\"data/vertical_arrows.npy\")  # shape (N, 784)\n",
    "\n",
    "# Pick the 4th sample (index 3)\n",
    "img_flat = arr[10]\n",
    "\n",
    "# Choose a random rotation angle (0–360 degrees)\n",
    "angle = np.random.uniform(0, 360)\n",
    "\n",
    "# Rotate the image\n",
    "img_rotated = rotate(img_flat.reshape(28,28), angle, reshape=False, order=1, mode='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205799a",
   "metadata": {},
   "source": [
    "### Display the rotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15a26331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation angle: 2.9878505326418603\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(img_rotated, cmap='gray')\n",
    "plt.title(f\"Rotated image by {angle:.1f} degrees\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Rotation angle:\", angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5fb4add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.01076689, 0.0106689 , 0.01047701, 0.00865637, 0.90680146,\n",
      "        0.04228568, 0.01034373]], dtype=float32), array([[-51.576534, -51.560356]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_class, pred_orient = predict_and_show(img_rotated, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"shapes_tinycnn_keras_plus_arrows.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now go to terminal to make a js model\n",
    "!tensorflowjs_converter --input_format=keras \\\n",
    "    shapes_tinycnn_keras_plus_vertical_line.h5 \\\n",
    "    web_model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doodle-autocomplete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
